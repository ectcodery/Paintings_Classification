{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 256, 256, 3) (283, 11) (122, 256, 256, 3) (122, 11)\n",
      "283 train samples\n",
      "122 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 250, 250, 64)      9472      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 35, 35, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 33, 33, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 11)                42603     \n",
      "=================================================================\n",
      "Total params: 70,795\n",
      "Trainable params: 70,667\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 212 samples, validate on 71 samples\n",
      "Epoch 1/35\n",
      "212/212 [==============================] - 4s 19ms/step - loss: 10.6300 - acc: 0.1462 - val_loss: 8.0602 - val_acc: 0.3803\n",
      "Epoch 2/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 8.4824 - acc: 0.3585 - val_loss: 7.6481 - val_acc: 0.5070\n",
      "Epoch 3/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 7.4844 - acc: 0.4623 - val_loss: 6.6536 - val_acc: 0.5070\n",
      "Epoch 4/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 6.5849 - acc: 0.5047 - val_loss: 6.4143 - val_acc: 0.4789\n",
      "Epoch 5/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 5.8406 - acc: 0.6274 - val_loss: 5.6216 - val_acc: 0.6197\n",
      "Epoch 6/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 5.2187 - acc: 0.6698 - val_loss: 5.4388 - val_acc: 0.5352\n",
      "Epoch 7/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 4.8593 - acc: 0.7123 - val_loss: 5.3736 - val_acc: 0.4085\n",
      "Epoch 8/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 4.2969 - acc: 0.7736 - val_loss: 4.6912 - val_acc: 0.6620\n",
      "Epoch 9/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 3.9188 - acc: 0.8019 - val_loss: 4.4559 - val_acc: 0.5211\n",
      "Epoch 10/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 3.4822 - acc: 0.8208 - val_loss: 3.9168 - val_acc: 0.6338\n",
      "Epoch 11/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 3.2319 - acc: 0.8160 - val_loss: 4.1720 - val_acc: 0.5775\n",
      "Epoch 12/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 2.9756 - acc: 0.7925 - val_loss: 3.6853 - val_acc: 0.5634\n",
      "Epoch 13/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 2.6667 - acc: 0.8679 - val_loss: 3.6355 - val_acc: 0.5775\n",
      "Epoch 14/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 2.4503 - acc: 0.8443 - val_loss: 3.4669 - val_acc: 0.5634\n",
      "Epoch 15/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 2.2129 - acc: 0.8538 - val_loss: 3.1053 - val_acc: 0.5352\n",
      "Epoch 16/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.9930 - acc: 0.9009 - val_loss: 2.9344 - val_acc: 0.6056\n",
      "Epoch 17/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.8703 - acc: 0.8774 - val_loss: 2.7606 - val_acc: 0.5211\n",
      "Epoch 18/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.6945 - acc: 0.8915 - val_loss: 2.8383 - val_acc: 0.5211\n",
      "Epoch 19/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.5810 - acc: 0.9198 - val_loss: 2.5105 - val_acc: 0.5915\n",
      "Epoch 20/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.3914 - acc: 0.9387 - val_loss: 2.9147 - val_acc: 0.5070\n",
      "Epoch 21/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.3576 - acc: 0.9104 - val_loss: 2.4011 - val_acc: 0.5915\n",
      "Epoch 22/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.1849 - acc: 0.9528 - val_loss: 2.4476 - val_acc: 0.5634\n",
      "Epoch 23/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.1119 - acc: 0.9528 - val_loss: 2.0436 - val_acc: 0.6479\n",
      "Epoch 24/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.0450 - acc: 0.9575 - val_loss: 2.4185 - val_acc: 0.5211\n",
      "Epoch 25/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.0679 - acc: 0.9434 - val_loss: 2.1484 - val_acc: 0.5915\n",
      "Epoch 26/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9176 - acc: 0.9387 - val_loss: 2.3251 - val_acc: 0.5634\n",
      "Epoch 27/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 0.9213 - acc: 0.9434 - val_loss: 1.9738 - val_acc: 0.6056\n",
      "Epoch 28/35\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.7704 - acc: 0.9575 - val_loss: 2.1101 - val_acc: 0.5493\n",
      "Epoch 29/35\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.7597 - acc: 0.9528 - val_loss: 1.9226 - val_acc: 0.6197\n",
      "Epoch 30/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 0.7349 - acc: 0.9434 - val_loss: 1.9911 - val_acc: 0.5634\n",
      "Epoch 31/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 0.6420 - acc: 0.9717 - val_loss: 2.0274 - val_acc: 0.5915\n",
      "Epoch 32/35\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 0.6054 - acc: 0.9858 - val_loss: 1.7586 - val_acc: 0.5915\n",
      "Epoch 33/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.5410 - acc: 1.0000 - val_loss: 1.8693 - val_acc: 0.6479\n",
      "Epoch 34/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.5450 - acc: 0.9764 - val_loss: 2.0228 - val_acc: 0.6197\n",
      "Epoch 35/35\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.5240 - acc: 0.9717 - val_loss: 1.6049 - val_acc: 0.5915\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "\n",
      "--------------------------------------\n",
      "Test loss: 1.4217282904953252\n",
      "Test accuracy: 0.6967213153839111\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 11\n",
    "epochs = 35\n",
    "image_size = 256\n",
    "\n",
    "def load_data():\n",
    "    train_label = np.load('train_label.npy')\n",
    "    train_input = np.load('train_input.npy')\n",
    "    train_label = np_utils.to_categorical(train_label, 11)\n",
    "    x_train,x_test,y_train, y_test = train_test_split(train_input, train_label, \n",
    "                                                      test_size=0.3)\n",
    "    x_train = x_train.astype('float')\n",
    "    x_test = x_test.astype('float')\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# 归一化\n",
    "x_train = x_train/255 - 0.5\n",
    "x_test = x_test/255 - 0.5\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, (7, 7), input_shape=(image_size, image_size, 3), \n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling2D(7, 7))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(normalization.BatchNormalization())\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(3, 3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(0.3),\n",
    "                 activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.25)\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('\\n--------------------------------------\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9472"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 256, 256, 3) (283, 11) (122, 256, 256, 3) (122, 11)\n",
      "283 train samples\n",
      "122 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_39 (Conv2D)           (None, 250, 250, 64)      9472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 250, 250, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 250, 250, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 33, 33, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 33, 33, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 11)                42603     \n",
      "=================================================================\n",
      "Total params: 70,923\n",
      "Trainable params: 70,731\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 212 samples, validate on 71 samples\n",
      "Epoch 1/35\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,250,250,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_18/Adam/gradients/zeros_5-0-1-TransposeNCHWToNHWC-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_18/Adam/gradients/zeros_5, PermConstNCHWToNHWC-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_18/Adam/gradients/AddN/_3017 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_551_training_18/Adam/gradients/AddN\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-8197bba7ee8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     validation_split=0.25)\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondains\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anacondains\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondains\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondains\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondains\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondains\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,250,250,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_18/Adam/gradients/zeros_5-0-1-TransposeNCHWToNHWC-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_18/Adam/gradients/zeros_5, PermConstNCHWToNHWC-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_18/Adam/gradients/AddN/_3017 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_551_training_18/Adam/gradients/AddN\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 11\n",
    "epochs = 35\n",
    "image_size = 256\n",
    "\n",
    "def load_data():\n",
    "    train_label = np.load('train_label.npy')\n",
    "    train_input = np.load('train_input.npy')\n",
    "    train_label = np_utils.to_categorical(train_label, 11)\n",
    "    x_train,x_test,y_train, y_test = train_test_split(train_input, train_label, \n",
    "                                                      test_size=0.3)\n",
    "    x_train = x_train.astype('float')\n",
    "    x_test = x_test.astype('float')\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# 归一化\n",
    "x_train = x_train/255 - 0.5\n",
    "x_test = x_test/255 - 0.5\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, (7, 7), input_shape=(image_size, image_size, 3), \n",
    "                        ))\n",
    "model.add(normalization.BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(7, 7))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(normalization.BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3)))\n",
    "model.add(normalization.BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(3, 3))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(0.5),\n",
    "                 activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.25)\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('\\n--------------------------------------\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 train samples\n",
      "102 test samples\n",
      "_________________________________________________________________"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anacondains\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(256, 256,..., activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "D:\\Anacondains\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_69 (Conv2D)           (None, 252, 252, 64)      4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 48, 48, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 11)                90123     \n",
      "=================================================================\n",
      "Total params: 113,707\n",
      "Trainable params: 113,579\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 212 samples, validate on 91 samples\n",
      "Epoch 1/34\n",
      "212/212 [==============================] - 7s 34ms/step - loss: 5.8303 - acc: 0.1226 - val_loss: 3.4757 - val_acc: 0.3626\n",
      "Epoch 2/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 3.8849 - acc: 0.3349 - val_loss: 3.0472 - val_acc: 0.4066\n",
      "Epoch 3/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 3.0139 - acc: 0.3774 - val_loss: 2.6579 - val_acc: 0.4835\n",
      "Epoch 4/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 2.6602 - acc: 0.5094 - val_loss: 2.5344 - val_acc: 0.5604\n",
      "Epoch 5/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 2.2593 - acc: 0.6368 - val_loss: 2.3990 - val_acc: 0.4945\n",
      "Epoch 6/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 2.0502 - acc: 0.6226 - val_loss: 2.1484 - val_acc: 0.6264\n",
      "Epoch 7/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.9343 - acc: 0.6934 - val_loss: 2.1744 - val_acc: 0.6044\n",
      "Epoch 8/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.6971 - acc: 0.7972 - val_loss: 2.2670 - val_acc: 0.6044\n",
      "Epoch 9/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.6670 - acc: 0.7406 - val_loss: 2.2593 - val_acc: 0.6374\n",
      "Epoch 10/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.6479 - acc: 0.7594 - val_loss: 2.2216 - val_acc: 0.6593\n",
      "Epoch 11/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.5438 - acc: 0.7972 - val_loss: 2.0470 - val_acc: 0.6813\n",
      "Epoch 12/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.5110 - acc: 0.8396 - val_loss: 2.0628 - val_acc: 0.6484\n",
      "Epoch 13/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.4193 - acc: 0.8302 - val_loss: 2.2876 - val_acc: 0.6264\n",
      "Epoch 14/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.3539 - acc: 0.8585 - val_loss: 2.0973 - val_acc: 0.6374\n",
      "Epoch 15/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.3134 - acc: 0.8868 - val_loss: 2.0297 - val_acc: 0.6813\n",
      "Epoch 16/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.2741 - acc: 0.9104 - val_loss: 1.9868 - val_acc: 0.7033\n",
      "Epoch 17/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.2367 - acc: 0.9009 - val_loss: 1.9529 - val_acc: 0.7143\n",
      "Epoch 18/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.1846 - acc: 0.9340 - val_loss: 2.0474 - val_acc: 0.6703\n",
      "Epoch 19/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.1762 - acc: 0.9387 - val_loss: 2.1339 - val_acc: 0.6154\n",
      "Epoch 20/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.1489 - acc: 0.9481 - val_loss: 2.1021 - val_acc: 0.6374\n",
      "Epoch 21/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.1279 - acc: 0.9387 - val_loss: 2.0530 - val_acc: 0.6593\n",
      "Epoch 22/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.1732 - acc: 0.9198 - val_loss: 1.9558 - val_acc: 0.6813\n",
      "Epoch 23/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.1898 - acc: 0.9292 - val_loss: 2.1567 - val_acc: 0.6703\n",
      "Epoch 24/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.1317 - acc: 0.9387 - val_loss: 2.1689 - val_acc: 0.6593\n",
      "Epoch 25/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 1.1063 - acc: 0.9528 - val_loss: 2.0662 - val_acc: 0.6703\n",
      "Epoch 26/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.0746 - acc: 0.9387 - val_loss: 2.0070 - val_acc: 0.6813\n",
      "Epoch 27/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 1.0165 - acc: 0.9575 - val_loss: 2.0982 - val_acc: 0.6593\n",
      "Epoch 28/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9857 - acc: 0.9858 - val_loss: 2.0845 - val_acc: 0.6484\n",
      "Epoch 29/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9936 - acc: 0.9764 - val_loss: 2.0421 - val_acc: 0.6484\n",
      "Epoch 30/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9630 - acc: 0.9858 - val_loss: 2.0615 - val_acc: 0.6703\n",
      "Epoch 31/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9566 - acc: 0.9811 - val_loss: 2.1346 - val_acc: 0.6703\n",
      "Epoch 32/34\n",
      "212/212 [==============================] - 2s 11ms/step - loss: 0.9378 - acc: 0.9764 - val_loss: 1.9761 - val_acc: 0.6813\n",
      "Epoch 33/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9267 - acc: 0.9858 - val_loss: 1.9321 - val_acc: 0.6703\n",
      "Epoch 34/34\n",
      "212/212 [==============================] - 2s 10ms/step - loss: 0.9253 - acc: 0.9811 - val_loss: 2.1163 - val_acc: 0.6813\n",
      "102/102 [==============================] - 1s 10ms/step\n",
      "Test loss: 1.9444199262880812\n",
      "Test accuracy: 0.6862745109726401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6,  3, 10,  0,  1,  6,  6,  1, 10,  6,  6,  8, 10,  2,  7, 10,  1,\n",
       "        5,  2,  9,  5,  0,  1,  0,  7, 10,  1,  8,  0, 10,  0,  5,  1,  4,\n",
       "        4,  5, 10,  8,  3,  2,  1,  8, 10,  8,  0, 10], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 11\n",
    "epochs = 34\n",
    "image_size = 256\n",
    "\n",
    "def load_data():\n",
    "    train_label = np.load('train_label.npy')\n",
    "    train_input = np.load('train_input.npy')\n",
    "    train_label = np_utils.to_categorical(train_label, 11)\n",
    "    x_train,x_test,y_train, y_test = train_test_split(train_input, train_label, \n",
    "                                                      test_size=0.25)\n",
    "    x_train = x_train.astype('float')\n",
    "    x_test = x_test.astype('float')\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# 归一化\n",
    "x_train = x_train/128 - 1\n",
    "x_test = x_test/128 - 1\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, (5, 5), input_shape=(image_size, image_size, 3), \n",
    "                        activation='relu', init='uniform'))\n",
    "model.add(MaxPooling2D(5, 5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(normalization.BatchNormalization(axis=-1))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', init='uniform'))\n",
    "model.add(MaxPooling2D(3, 3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(0.05),\n",
    "                activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# plot_model(model, to_file='model.png',show_shapes='True')\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.3)\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "test_input = np.load('test_input.npy')\n",
    "y = model.predict(test_input/128 - 1)\n",
    "np.save('out__%d.npy'%(score[1]*100), y)\n",
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 train samples\n",
      "41 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 252, 252, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 50, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 48, 48, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                90123     \n",
      "=================================================================\n",
      "Total params: 96,043\n",
      "Trainable params: 96,011\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anacondains\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), input_shape=(256, 256,..., activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "D:\\Anacondains\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 254 samples, validate on 110 samples\n",
      "Epoch 1/70\n",
      "254/254 [==============================] - 18s 69ms/step - loss: 4.9258 - acc: 0.1181 - val_loss: 4.0720 - val_acc: 0.3545\n",
      "Epoch 2/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 4.1914 - acc: 0.3780 - val_loss: 3.7112 - val_acc: 0.3909\n",
      "Epoch 3/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 3.5415 - acc: 0.4606 - val_loss: 3.5652 - val_acc: 0.3727\n",
      "Epoch 4/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 3.1555 - acc: 0.5787 - val_loss: 3.3590 - val_acc: 0.3727\n",
      "Epoch 5/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 2.9137 - acc: 0.6260 - val_loss: 3.0589 - val_acc: 0.5545\n",
      "Epoch 6/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 2.7069 - acc: 0.6496 - val_loss: 2.8947 - val_acc: 0.5364\n",
      "Epoch 7/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 2.4682 - acc: 0.7244 - val_loss: 2.8605 - val_acc: 0.4818\n",
      "Epoch 8/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 2.3362 - acc: 0.7480 - val_loss: 2.6215 - val_acc: 0.6091\n",
      "Epoch 9/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 2.1776 - acc: 0.7756 - val_loss: 2.5354 - val_acc: 0.6091\n",
      "Epoch 10/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.9878 - acc: 0.8425 - val_loss: 2.4810 - val_acc: 0.5818\n",
      "Epoch 11/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.8878 - acc: 0.8228 - val_loss: 2.3176 - val_acc: 0.6000\n",
      "Epoch 12/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.7938 - acc: 0.8307 - val_loss: 2.2759 - val_acc: 0.6273\n",
      "Epoch 13/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.6891 - acc: 0.8976 - val_loss: 2.2308 - val_acc: 0.6364\n",
      "Epoch 14/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.5946 - acc: 0.9134 - val_loss: 2.0833 - val_acc: 0.6545\n",
      "Epoch 15/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.4899 - acc: 0.9094 - val_loss: 2.0301 - val_acc: 0.7000\n",
      "Epoch 16/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 1.3915 - acc: 0.9331 - val_loss: 2.0558 - val_acc: 0.6636\n",
      "Epoch 17/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 1.3506 - acc: 0.9291 - val_loss: 1.9411 - val_acc: 0.6818\n",
      "Epoch 18/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 1.2715 - acc: 0.9370 - val_loss: 1.8996 - val_acc: 0.6909\n",
      "Epoch 19/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.2304 - acc: 0.9331 - val_loss: 1.9118 - val_acc: 0.6545\n",
      "Epoch 20/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.1778 - acc: 0.9370 - val_loss: 1.9043 - val_acc: 0.6727\n",
      "Epoch 21/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.1368 - acc: 0.9449 - val_loss: 1.8277 - val_acc: 0.7091\n",
      "Epoch 22/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 1.0759 - acc: 0.9528 - val_loss: 1.8249 - val_acc: 0.6909\n",
      "Epoch 23/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 1.0458 - acc: 0.9528 - val_loss: 1.7541 - val_acc: 0.6909\n",
      "Epoch 24/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.9710 - acc: 0.9724 - val_loss: 1.7999 - val_acc: 0.6182\n",
      "Epoch 25/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.9262 - acc: 0.9803 - val_loss: 1.7314 - val_acc: 0.6909\n",
      "Epoch 26/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.9004 - acc: 0.9803 - val_loss: 1.6924 - val_acc: 0.6909\n",
      "Epoch 27/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.8666 - acc: 0.9843 - val_loss: 1.6230 - val_acc: 0.7000\n",
      "Epoch 28/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.8629 - acc: 0.9646 - val_loss: 1.5444 - val_acc: 0.6909\n",
      "Epoch 29/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.8127 - acc: 0.9764 - val_loss: 1.5949 - val_acc: 0.6545\n",
      "Epoch 30/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.8385 - acc: 0.9488 - val_loss: 1.6392 - val_acc: 0.6545\n",
      "Epoch 31/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.7627 - acc: 0.9803 - val_loss: 1.4664 - val_acc: 0.7000\n",
      "Epoch 32/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.7129 - acc: 0.9764 - val_loss: 1.5465 - val_acc: 0.6636\n",
      "Epoch 33/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.6924 - acc: 0.9803 - val_loss: 1.5005 - val_acc: 0.6727\n",
      "Epoch 34/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.6616 - acc: 0.9961 - val_loss: 1.4199 - val_acc: 0.6818\n",
      "Epoch 35/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.6196 - acc: 0.9961 - val_loss: 1.4427 - val_acc: 0.6727\n",
      "Epoch 36/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.6051 - acc: 0.9961 - val_loss: 1.4496 - val_acc: 0.6727\n",
      "Epoch 37/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.5969 - acc: 0.9843 - val_loss: 1.3624 - val_acc: 0.6818\n",
      "Epoch 38/70\n",
      "254/254 [==============================] - 2s 6ms/step - loss: 0.5819 - acc: 0.9921 - val_loss: 1.3698 - val_acc: 0.6909\n",
      "Epoch 39/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.5475 - acc: 0.9921 - val_loss: 1.3754 - val_acc: 0.6818\n",
      "Epoch 40/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.5351 - acc: 0.9961 - val_loss: 1.3330 - val_acc: 0.7000\n",
      "Epoch 41/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.5119 - acc: 1.0000 - val_loss: 1.3331 - val_acc: 0.7273\n",
      "Epoch 42/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4963 - acc: 0.9961 - val_loss: 1.3356 - val_acc: 0.6818\n",
      "Epoch 43/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4946 - acc: 0.9882 - val_loss: 1.3109 - val_acc: 0.6818\n",
      "Epoch 44/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4598 - acc: 1.0000 - val_loss: 1.3029 - val_acc: 0.7000\n",
      "Epoch 45/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4646 - acc: 0.9882 - val_loss: 1.2456 - val_acc: 0.7182\n",
      "Epoch 46/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4413 - acc: 0.9921 - val_loss: 1.2676 - val_acc: 0.6909\n",
      "Epoch 47/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4335 - acc: 0.9921 - val_loss: 1.3076 - val_acc: 0.6818\n",
      "Epoch 48/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.4220 - acc: 0.9921 - val_loss: 1.2653 - val_acc: 0.6909\n",
      "Epoch 49/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3979 - acc: 0.9961 - val_loss: 1.2196 - val_acc: 0.7000\n",
      "Epoch 50/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3942 - acc: 0.9961 - val_loss: 1.2384 - val_acc: 0.7000\n",
      "Epoch 51/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3851 - acc: 1.0000 - val_loss: 1.2698 - val_acc: 0.6727\n",
      "Epoch 52/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3815 - acc: 0.9921 - val_loss: 1.2058 - val_acc: 0.6818\n",
      "Epoch 53/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3698 - acc: 0.9921 - val_loss: 1.2614 - val_acc: 0.6818\n",
      "Epoch 54/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3489 - acc: 1.0000 - val_loss: 1.1958 - val_acc: 0.7091\n",
      "Epoch 55/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3539 - acc: 0.9921 - val_loss: 1.1606 - val_acc: 0.7182\n",
      "Epoch 56/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3350 - acc: 1.0000 - val_loss: 1.2390 - val_acc: 0.6909\n",
      "Epoch 57/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3318 - acc: 0.9921 - val_loss: 1.1868 - val_acc: 0.6818\n",
      "Epoch 58/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3254 - acc: 0.9921 - val_loss: 1.1734 - val_acc: 0.6818\n",
      "Epoch 59/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3080 - acc: 1.0000 - val_loss: 1.2095 - val_acc: 0.6545\n",
      "Epoch 60/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.3043 - acc: 1.0000 - val_loss: 1.1226 - val_acc: 0.6909\n",
      "Epoch 61/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2958 - acc: 1.0000 - val_loss: 1.1182 - val_acc: 0.7091\n",
      "Epoch 62/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2920 - acc: 1.0000 - val_loss: 1.2051 - val_acc: 0.6727\n",
      "Epoch 63/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2854 - acc: 1.0000 - val_loss: 1.1427 - val_acc: 0.6727\n",
      "Epoch 64/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2830 - acc: 0.9921 - val_loss: 1.1080 - val_acc: 0.7182\n",
      "Epoch 65/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2720 - acc: 0.9961 - val_loss: 1.1487 - val_acc: 0.7000\n",
      "Epoch 66/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2709 - acc: 0.9961 - val_loss: 1.1161 - val_acc: 0.7273\n",
      "Epoch 67/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2641 - acc: 1.0000 - val_loss: 1.0796 - val_acc: 0.7182\n",
      "Epoch 68/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2502 - acc: 1.0000 - val_loss: 1.0874 - val_acc: 0.7000\n",
      "Epoch 69/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2374 - acc: 1.0000 - val_loss: 1.1356 - val_acc: 0.6909\n",
      "Epoch 70/70\n",
      "254/254 [==============================] - 1s 6ms/step - loss: 0.2417 - acc: 1.0000 - val_loss: 1.1041 - val_acc: 0.7000\n",
      "41/41 [==============================] - 0s 10ms/step\n",
      "Test loss: 0.7900267737667735\n",
      "Test accuracy: 0.8292682926829268\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 11\n",
    "epochs = 70\n",
    "image_size = 256\n",
    "\n",
    "def load_data():\n",
    "    train_label = np.load('train_label.npy')\n",
    "    train_input = np.load('train_input.npy')\n",
    "    train_label = np_utils.to_categorical(train_label, 11)\n",
    "    x_train,x_test,y_train, y_test = train_test_split(train_input, train_label, \n",
    "                                                      test_size=0.10)\n",
    "    x_train = x_train.astype('float')\n",
    "    x_test = x_test.astype('float')\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# 归一化\n",
    "x_train = x_train/128 - 1\n",
    "x_test = x_test/128 - 1\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, (5, 5), input_shape=(image_size, image_size, 3), \n",
    "                        activation='relu', init='uniform'))\n",
    "model.add(MaxPooling2D(5, 5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(normalization.BatchNormalization(axis=-1))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', init='uniform'))\n",
    "model.add(MaxPooling2D(3, 3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(0.1),\n",
    "                activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# plot_model(model, to_file='model.png',show_shapes='True')\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.3)\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anacondains\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 train samples\n",
      "102 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anacondains\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5), input_shape=(256, 256,..., activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "D:\\Anacondains\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 252, 252, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 50, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                90123     \n",
      "=================================================================\n",
      "Total params: 96,043\n",
      "Trainable params: 96,011\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "Train on 212 samples, validate on 91 samples\n",
      "Epoch 1/50\n",
      "212/212 [==============================] - 13s 60ms/step - loss: 4.9747 - acc: 0.1840 - val_loss: 4.1079 - val_acc: 0.3626\n",
      "Epoch 2/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 3.9890 - acc: 0.3774 - val_loss: 3.5455 - val_acc: 0.4615\n",
      "Epoch 3/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 3.3250 - acc: 0.5519 - val_loss: 3.3288 - val_acc: 0.4505\n",
      "Epoch 4/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 3.1253 - acc: 0.5236 - val_loss: 3.0867 - val_acc: 0.5275\n",
      "Epoch 5/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 2.7898 - acc: 0.6651 - val_loss: 2.9622 - val_acc: 0.5824\n",
      "Epoch 6/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 2.6567 - acc: 0.6509 - val_loss: 2.7980 - val_acc: 0.5824\n",
      "Epoch 7/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 2.4312 - acc: 0.7264 - val_loss: 2.7175 - val_acc: 0.5385\n",
      "Epoch 8/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 2.3036 - acc: 0.7170 - val_loss: 2.4944 - val_acc: 0.6374\n",
      "Epoch 9/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 2.0716 - acc: 0.7972 - val_loss: 2.3752 - val_acc: 0.6813\n",
      "Epoch 10/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.9751 - acc: 0.8113 - val_loss: 2.3433 - val_acc: 0.6593\n",
      "Epoch 11/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.8900 - acc: 0.8349 - val_loss: 2.2863 - val_acc: 0.6264\n",
      "Epoch 12/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.7484 - acc: 0.8679 - val_loss: 2.2725 - val_acc: 0.6264\n",
      "Epoch 13/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.6534 - acc: 0.8821 - val_loss: 2.0313 - val_acc: 0.6813\n",
      "Epoch 14/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.5824 - acc: 0.9104 - val_loss: 1.9422 - val_acc: 0.7473\n",
      "Epoch 15/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.4627 - acc: 0.9292 - val_loss: 1.9537 - val_acc: 0.6813\n",
      "Epoch 16/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.3886 - acc: 0.9151 - val_loss: 1.8721 - val_acc: 0.7473\n",
      "Epoch 17/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.3250 - acc: 0.9245 - val_loss: 1.8044 - val_acc: 0.7363\n",
      "Epoch 18/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.2669 - acc: 0.9481 - val_loss: 1.7462 - val_acc: 0.7473\n",
      "Epoch 19/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.2146 - acc: 0.9575 - val_loss: 1.7062 - val_acc: 0.7033\n",
      "Epoch 20/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.1552 - acc: 0.9623 - val_loss: 1.6728 - val_acc: 0.7363\n",
      "Epoch 21/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.1256 - acc: 0.9764 - val_loss: 1.5947 - val_acc: 0.7473\n",
      "Epoch 22/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 1.0492 - acc: 0.9811 - val_loss: 1.5504 - val_acc: 0.8132\n",
      "Epoch 23/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.9950 - acc: 0.9858 - val_loss: 1.5793 - val_acc: 0.7473\n",
      "Epoch 24/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.9712 - acc: 0.9670 - val_loss: 1.5388 - val_acc: 0.7253\n",
      "Epoch 25/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.9520 - acc: 0.9670 - val_loss: 1.4751 - val_acc: 0.7802\n",
      "Epoch 26/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.9383 - acc: 0.9623 - val_loss: 1.4418 - val_acc: 0.7582\n",
      "Epoch 27/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.8613 - acc: 0.9858 - val_loss: 1.5402 - val_acc: 0.6923\n",
      "Epoch 28/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.8733 - acc: 0.9717 - val_loss: 1.3628 - val_acc: 0.8132\n",
      "Epoch 29/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.8338 - acc: 0.9528 - val_loss: 1.3304 - val_acc: 0.7692\n",
      "Epoch 30/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.7954 - acc: 0.9811 - val_loss: 1.4301 - val_acc: 0.7143\n",
      "Epoch 31/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.7487 - acc: 0.9906 - val_loss: 1.2848 - val_acc: 0.7912\n",
      "Epoch 32/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.7409 - acc: 0.9717 - val_loss: 1.2578 - val_acc: 0.7912\n",
      "Epoch 33/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.6904 - acc: 0.9953 - val_loss: 1.3754 - val_acc: 0.7363\n",
      "Epoch 34/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.6631 - acc: 0.9953 - val_loss: 1.3167 - val_acc: 0.7143\n",
      "Epoch 35/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.6486 - acc: 0.9906 - val_loss: 1.2453 - val_acc: 0.7912\n",
      "Epoch 36/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.6398 - acc: 0.9906 - val_loss: 1.2102 - val_acc: 0.8022\n",
      "Epoch 37/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.6123 - acc: 0.9953 - val_loss: 1.1917 - val_acc: 0.7473\n",
      "Epoch 38/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.6104 - acc: 0.9858 - val_loss: 1.1787 - val_acc: 0.7473\n",
      "Epoch 39/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.5686 - acc: 0.9953 - val_loss: 1.2241 - val_acc: 0.6923\n",
      "Epoch 40/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.5551 - acc: 0.9953 - val_loss: 1.1847 - val_acc: 0.7692\n",
      "Epoch 41/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.5417 - acc: 1.0000 - val_loss: 1.1123 - val_acc: 0.7912\n",
      "Epoch 42/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.5311 - acc: 1.0000 - val_loss: 1.1059 - val_acc: 0.7692\n",
      "Epoch 43/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.5147 - acc: 0.9953 - val_loss: 1.1984 - val_acc: 0.7363\n",
      "Epoch 44/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.4967 - acc: 0.9953 - val_loss: 1.1819 - val_acc: 0.7253\n",
      "Epoch 45/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.4885 - acc: 0.9953 - val_loss: 1.0892 - val_acc: 0.7802\n",
      "Epoch 46/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.4568 - acc: 1.0000 - val_loss: 1.3057 - val_acc: 0.6813\n",
      "Epoch 47/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.5079 - acc: 0.9764 - val_loss: 1.1230 - val_acc: 0.7253\n",
      "Epoch 48/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.4561 - acc: 0.9953 - val_loss: 1.1273 - val_acc: 0.7253\n",
      "Epoch 49/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.4393 - acc: 0.9953 - val_loss: 1.0835 - val_acc: 0.7033\n",
      "Epoch 50/50\n",
      "212/212 [==============================] - 1s 6ms/step - loss: 0.4222 - acc: 0.9953 - val_loss: 1.1561 - val_acc: 0.6813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 1s 6ms/step\n",
      "Test loss: 1.3013978378445494\n",
      "Test accuracy: 0.6470588223606932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6,  3,  3,  0,  1,  6,  6,  1,  3,  6,  6,  8, 10,  0,  4, 10,  1,\n",
       "        5,  2,  0,  9,  0,  8,  0,  9, 10,  1,  8,  0,  7,  0,  5,  1,  6,\n",
       "        4,  9, 10,  8,  0,  2,  1,  8, 10,  8,  0,  4], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 11\n",
    "epochs = 50\n",
    "image_size = 256\n",
    "\n",
    "def load_data():\n",
    "    train_label = np.load('train_label.npy')\n",
    "    train_input = np.load('train_input.npy')\n",
    "    train_label = np_utils.to_categorical(train_label, 11)\n",
    "    x_train,x_test,y_train, y_test = train_test_split(train_input, train_label, \n",
    "                                                      test_size=0.25)\n",
    "    x_train = x_train.astype('float')\n",
    "    x_test = x_test.astype('float')\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# 归一化\n",
    "x_train = x_train/128 - 1\n",
    "x_test = x_test/128 - 1\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, (5, 5), input_shape=(image_size, image_size, 3), \n",
    "                        activation='relu', init='uniform'))\n",
    "model.add(MaxPooling2D(5, 5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(normalization.BatchNormalization(axis=-1))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', init='uniform'))\n",
    "model.add(MaxPooling2D(3, 3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(0.1),\n",
    "                activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# plot_model(model, to_file='model.png',show_shapes='True')\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.3)\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "test_input = np.load('test_input.npy')\n",
    "y = model.predict(test_input/128 - 1)\n",
    "np.save('regulartion==0.05  %d.npy'%(score[1]*100), y)\n",
    "np.argmax(y, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3*25+1)*16\n",
    "16*16*32*11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*(9*3+1)*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4640/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e38182346174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m73\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'factor' is not defined"
     ]
    }
   ],
   "source": [
    "factor(73)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
